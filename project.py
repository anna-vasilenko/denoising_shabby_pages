# -*- coding: utf-8 -*-
"""проектик.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L7HNBSGkzEi7M3CUIB4uX81_rNhRmsQh
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import zipfile
import os
import cv2

from sklearn.model_selection import train_test_split
import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout, BatchNormalization, Input
from tensorflow.keras.callbacks import EarlyStopping

# %matplotlib inline

!ls "/content/drive/MyDrive/Colab Notebooks/project_DL/try_2"

from google.colab import drive
drive.mount('/content/drive')
os.chdir("/content/drive/MyDrive/Colab Notebooks/project_DL/try_2")
!ls

path_zip = 'zip/'
path = 'data/'

with zipfile.ZipFile('train.zip', 'r') as zip_ref:
    zip_ref.extractall('train')

with zipfile.ZipFile('test.zip', 'r') as zip_ref:
    zip_ref.extractall('test')

with zipfile.ZipFile('train_cleaned.zip', 'r') as zip_ref:
    zip_ref.extractall('train_cleaned')

train_img = sorted(os.listdir('train/train'))
train_cleaned_img = sorted(os.listdir('train_cleaned/train_cleaned'))
test_img = sorted(os.listdir('test/test'))

"""### Подготовка данных

Следующий шаг — определить функцию для обработки изображений, а затем сохранить эти изображения в список. Так как данных не так много, нет необходимости работать с ними пакетами (batch'ами).

"""

IMG_WIDTH = 540
IMG_HEIGHT = 420

def process_image(path):
    img = cv2.imread(path)
    if img is None:
        raise ValueError(f"Не удалось загрузить изображение: {path}")
    img = np.asarray(img, dtype="float32")
    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))
    if len(img.shape) == 3 and img.shape[2] == 3:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = img / 255.0
    img = np.reshape(img, (IMG_HEIGHT, IMG_WIDTH, 1))
    return img

train = []
train_cleaned = []
test = []

for f in train_img:
    train.append(process_image(os.path.join('train/train', f)))

for f in train_cleaned_img:
    train_cleaned.append(process_image(os.path.join('train_cleaned/train_cleaned',f)))

for f in test_img:
    test.append(process_image(os.path.join('test/test',f)))

plt.figure(figsize=(15,25))
for i in range(0,8,2):
    plt.subplot(4,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(train[i][:,:,0], cmap='gray')
    plt.title('Noise image: {}'.format(train_img[i]))

    plt.subplot(4,2,i+2)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(train_cleaned[i][:,:,0], cmap='gray')
    plt.title('Denoised image: {}'.format(train_img[i]))

plt.show()

"""Попробуем добавить валидации к этому делу"""

X_train = np.asarray(train)
Y_train = np.asarray(train_cleaned)
X_test = np.asarray(test)

X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15)

"""Используется оптимизатор Adam (так как он показал наилучшие результаты среди других оптимизаторов), функция потерь основана на среднем квадрате ошибки, а также отслеживаем среднюю абсолютную ошибку."""

def model():
    input_layer = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1))

    # encoding
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)

    x = MaxPooling2D((2, 2), padding='same')(x)

    x = Dropout(0.5)(x)

    # decoding
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)

    x = UpSampling2D((2, 2))(x)

    output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
    model = Model(inputs=[input_layer], outputs=[output_layer])
    model.compile(optimizer='adam' , loss='mean_squared_error', metrics=['mae'])

    return model


model = model()
model.summary()

callback = EarlyStopping(monitor='loss', patience=10)
history = model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=80, batch_size=12, verbose=1, callbacks=[callback])

model.save_weights('my_checkpoint.weights.h5')

# Check how loss & mae went down
epoch_loss = history.history['loss']
epoch_val_loss = history.history['val_loss']
epoch_mae = history.history['mae']
epoch_val_mae = history.history['val_mae']

plt.figure(figsize=(20,6))
plt.subplot(1,2,1)
plt.plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss')
plt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val Loss')
plt.title('Evolution of loss on train & validation datasets over epochs')
plt.legend(loc='best')

plt.subplot(1,2,2)
plt.plot(range(0,len(epoch_mae)), epoch_mae, 'b-', linewidth=2, label='Train MAE')
plt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, 'r-', linewidth=2,label='Val MAE')
plt.title('Evolution of MAE on train & validation datasets over epochs')
plt.legend(loc='best')

plt.show()

Y_test = model.predict(X_test[:4])

plt.figure(figsize=(15,25))

for i in range(0,8,2):
  index = int(i/2)
  plt.subplot(4,2,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(X_test[index][:,:,0], cmap='gray')
  plt.title('Noisy image: {}'.format(test_img[index]))

  plt.subplot(4,2,i+2)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(Y_test[index][:,:,0], cmap='gray')
  plt.title('Denoised by autoencoder: {}'.format(test_img[index]))

plt.show()